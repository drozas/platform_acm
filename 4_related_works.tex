% !TEX root = 0_main.tex

\section{Related Works}\label{sec:related_works}
Various alternatives have been proposed in the literature to allocate crowdsourcing tasks \cite{machado2016task, cui2017complex, mao2015developer, ho2013adaptive, difallah2013pick, kamel2020tasks, zhao2019task, jiang2019group, yu2013bringing, fu2021fairness} as well as to re-think the conditions of crowdsourced labour more generally \cite{kittur2013future,graham2018towards}. Some authors have suggested implementing the power of AI techniques to assign tasks to workers, while other scholars have introduced reputation schemes to delegate tasks. Workers' background, expertise, and social connections have also been considered in approaches presented to improve the assignment of tasks in crowdsourcing platforms.% Others have drawn on distributed computing to re-envisioning crowd work that ``we would want our children to participate in.'' Based on surveying AMT workers, Kittur et al. proposed a new framework for crowd work focused on supporting complex, high-quality work \cite{kittur2013future}. Furthermore, some researchers have envisioned criteria to distinguish fair from exploitative platforms \cite{graham2018towards}.% Along this line, McInnis et al. \cite{mcinnis2016taking} as well as Brawley and Pury \cite{brawley2016work} study the job satisfaction and workers' experiences in crowdsourcing. 

In Machado et al. \cite{machado2016task}, the authors suggest using AI planning to help choose the best delegation strategy based on parameters that configure the crowdsourcing environment, such as task duration and workers' skills. A machine learning-based approach that combines supervised learning with reinforcement learning to infer task allocation strategies that best fit the available rewards and workers' reputation is proposed by Cui and colleagues \cite{cui2017complex}.  A content-based recommendation method is introduced in Mao et al. \cite{mao2015developer} to match crowdsourced development tasks to developers automatically. The system learns from historical activities to favour appropriate workers. Ho et al. \cite{ho2013adaptive} present an algorithm to allocate tasks in situations of heterogeneous tasks or a diverse, skilled workforce.

Difallah and colleagues \cite{difallah2013pick} employ information available in the workers' social network profiles, such as their interests, to automatically assign workers to tasks aligned to them. The matching between workers and tasks is based on a taxonomy derived from categories extracted from workers' interests and descriptions of tasks. Likewise, the construction of workers' profiles using historical data of their performance and data extracted from social networks is suggested in Kamel et al. \cite{kamel2020tasks}. With these data, the authors propose the development of a machine learning model to recommend relevant tasks to workers based on their built profiles. Similarly, Micholia et al., [CITATION NEEDED] developed a framework for task allocation in mobile crowdsourcing marketplaces that relies on identifying users' expertise and interest in performing particular tasks by analyzing their social media profiles. Zhao et al. \cite{zhao2019task} also discuss a model that considers the relationship between workers to assign tasks in crowdsourcing. The proposal is to use social networking sites to learn about the social connections between workers and therefore allocate them and their friends the same or similar tasks.

The allocation of tasks to groups or teams of workers instead of individuals is explored in \cite{jiang2019group}. In group-oriented crowdsourcing, members of naturally existing groups of workers cooperate to perform tasks. Jiang et al. introduce, in this article, the concept of contextual crowdsourcing value, which determines the priority of a group of workers being allocated a task. The contextual crowdsourcing value measures the capacity of the group of workers to complete a given task in coordination with other groups that complement the missing skills of the group's members. Through experimenting with different approaches to gamified crowdsourcing Morschheusera et al. [CITATION NEEDED] found that users preferred crowdsourcing approaches that included cooperation, but found group-based competition to be the most engaging form of crowdsourcing.  

Reputation models for task allocation have been studied by \cite{yu2013bringing}. Here, workers' reputation is estimated based on the workers' past performance, considering the quality of previous work and meeting deadlines. Increasing fairness while reducing costs is proposed by Fu and Liu \cite{fu2021fairness} who introduce a task allocation model (F-Aware) to create fairer crowdsourcing workflows. The proposed approach monitors the execution of workflows, adjusting the operation of the allocation algorithm to achieve a fairer distribution of labour among workers.

Our work contributes a novel perspective of task allocation on crowdsourcing platforms. Instead of proposing an approach that targets cost reduction, budget balance, quality assurance, or timely completion optimisation, as in the reviewed literature, we report on alternative models that have the potential to help allocate tasks in a fairer way drawing on co-designing techniques which allow workers themselves to define task allocation models that improve their welfare in the platform. Previous research has also explored collaborations with workers of the platforms to explore alternatives to change the nature of crowdsourcing work. In response to concerns from AMT workers over a lack of employer accountability, Irani et al. \cite{irani2013turkopticon} developed ``Turkopticon.'' Turkopticon is a platform and browser extension where workers can share experiences about employers, allowing for greater transparency and communication among workers \cite{irani2013turkopticon}. As part of the tool, Turkopticon reveals workersâ€™ views of their task lists with information others have written about employers. AMT workers have also employed generic platforms, such as Reddit, to share advice and experiences of working on AMT \cite{martin2014being, zyskowski2018crowded}. Additionally, researchers in collaboration with AMT workers created a platform called Dynamo to support collective action \cite{salehi2015we}.

However, our study differs from theirs in co-designing directly with AOD workers after establishing a collaboration with the core team that controls and sustains the platform and its code. As discussed in Section \ref{sec:theoretical-framework}, AMT and AOD represent different forms of crowdsourcing platforms, showing us how platforms can increase the alienation of workers, but they can also help to reduce it \cite{hansson_capitalizing_2018}. In this sense, the core team of AOD is willing to experiment with alternative logics that provide more power to the workers themselves and integrate them into the primary platform. In the aforementioned studies \cite{irani2013turkopticon, salehi2015we}, on AMT workers co-designing alternative platforms, the platforms developed represent a form of counter-power, rather than control over the main platform. Next, we provide an overview of the methods employed to follow this co-designing approach.