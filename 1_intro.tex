% !TEX root = 0_main.tex

\section{Introduction}

Contemporary working practices are changing and digital platforms are increasingly mediating our day-to-day work. In this scenario, crowdsourced forms of labour are progressively gaining importance, and large corporations such as Amazon and Microsoft are entering the field. Amazon Mechanical Turk (AMT), Universal Human Relevance System (UHRS), and TaskRabbit are all examples of market-driven crowdsourcing platforms. These platforms operate as labour marketplaces for businesses to outsource work to globally distributed and diverse workers \cite{10.1145/1753846.1753873}. %Upwork is a freelancing marketplace in which customers provide a description of the task and a price range so that freelancers can offer their services. As of 2017 \cite{carrel-billiard_2017}, it had five million registered clients who were annually posting three million jobs to twelve million registered freelancers that were reporting more than US\$1 billion in earnings.

In crowdsourcing platforms, work is ``taskified'' \cite{amer2016toward}. Entering receipts into expense reports, curating data to train an Artificial Intelligence (AI) model, translating texts and tagging words and images, are examples of work that can be easily ``taskified''. These tasks are carried out by an invisible workforce of ``humans in the loop'' \cite{gray2019ghost}. Through platforms such as AMT, in this volatile and globally distributed crowd of workers, with varying degrees of expertise and backgrounds \cite{10.1145/1753846.1753873}, people compete against each other to find tasks to work on. The logic behind the allocation of these tasks typically operates on a ``First-Come, First-Served'' (FCFS) basis \cite{yu2013bringing, han2019all}.

Previous research has argued that FCFS is a convenient method of task allocation because of its simplicity and capacity to decrease task completion time \cite{kamel2020tasks}. The approach, however, creates a competitive dynamic in which workers are forced to be constantly alert for new tasks to appear, producing a sense of anxiety and frustration in case they cannot obtain the work \cite{gray2019ghost}. Additionally, FCFS disadvantages workers who do not have access to a reliable Internet connection or those who work in time zones different from the requesters.
Thus, it can create inequitable work distribution by relying on circumstances that are often beyond workers' control. 

Alternatives of work distribution have been proposed in crowdsourcing literature to optimize worker-task matching, maximising the task-requesters' benefits, and improving results (e.g., \cite{difallah2013pick, ho2012online, karger2014budget, yin2017task}), yet, besides some empirical studies that examine crowdsourcing issues from the workers' perspective (e.g., \cite{mcinnis2016taking, brawley2016work, faullant2013fair}), there is a lack of proposals which aim to improve the working conditions and well-being of workers. This article reports the preliminary results of an interactive design approach where workers have been involved throughout a research process that includes a variety of methods and which aims to identify and validate alternative task allocation logics defined and agreed upon by the workers themselves.

Our vision towards more worker-centric crowdsourcing platforms is summarised by the motto: \textit{``the platform belongs to those who work on it\footnote{We have adapted this motto inspired by Teodoro Flores's phrase ``la tierra es para quien la trabaja'' (``the land belongs to those who work it''), which captures the revolutionaries' vision for land reform in the context of the Mexican Revolution (1910-1920) \cite{mcneely1966origins}.}''}, which aims at empowering workers to define the rules that govern the distribution of value in crowdsourcing platforms.

The remainder of the paper proceeds as follows. Next, we present our case study, followed by a description of the theoretical concepts that frame the work and a review of related works. Section \ref{sec:methods} introduces the methods employed in the study. Later, Section \ref{sec:results} describes the results. A general discussion about the implications of the results is provided in Section \ref{sec:discussion}. We close the paper by presenting conclusions in Section \ref{sec:conclusion}.